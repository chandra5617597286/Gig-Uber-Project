{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8cd1d746",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17945e30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gender column has been added.\n",
      "vehicle_type column has been added.\n",
      "coordinate columns have been added.\n",
      "enhanced dataset has been saved to: data/enhanced_gig_data.csv\n"
     ]
    }
   ],
   "source": [
    "# loading the original or previously enhanced dataset\n",
    "df = pd.read_csv(\"data/rideshare_kaggle.csv\")\n",
    "\n",
    "# checking for gender column and adding if missing\n",
    "if 'gender' not in df.columns:\n",
    "    # adding gender column by choosing male or female randomly\n",
    "    df['gender'] = np.random.choice(['Male', 'Female'], size=len(df))\n",
    "    print(\"gender column has been added.\")\n",
    "else:\n",
    "    # saying that gender column is already there\n",
    "    print(\"gender column already exists.\")\n",
    "\n",
    "# checking for vehicle_type column and adding if missing\n",
    "if 'vehicle_type' not in df.columns:\n",
    "    # adding vehicle_type column by picking a type randomly\n",
    "    df['vehicle_type'] = np.random.choice(['Sedan', 'SUV', 'Bike', 'Hatchback'], size=len(df))\n",
    "    print(\"vehicle_type column has been added.\")\n",
    "else:\n",
    "    # saying that vehicle_type column is already there\n",
    "    print(\"vehicle_type column already exists.\")\n",
    "\n",
    "# defining location to coordinate mapping\n",
    "location_coords = {\n",
    "    'Back Bay': (42.3503, -71.0810),\n",
    "    'Beacon Hill': (42.3588, -71.0707),\n",
    "    'North End': (42.3656, -71.0542),\n",
    "    'North Station': (42.3664, -71.0622),\n",
    "    'South Station': (42.3522, -71.0555),\n",
    "    'Theatre District': (42.3519, -71.0645),\n",
    "    'West End': (42.3655, -71.0661),\n",
    "    'Financial District': (42.3550, -71.0552)\n",
    "}\n",
    "\n",
    "# checking and adding coordinate columns if not present\n",
    "if 'source_lat' not in df.columns:\n",
    "    # mapping source names to latitude and longitude\n",
    "    df['source_lat'] = df['source'].map(lambda x: location_coords.get(x, (np.nan, np.nan))[0])\n",
    "    df['source_lon'] = df['source'].map(lambda x: location_coords.get(x, (np.nan, np.nan))[1])\n",
    "    # mapping destination names to latitude and longitude\n",
    "    df['dest_lat'] = df['destination'].map(lambda x: location_coords.get(x, (np.nan, np.nan))[0])\n",
    "    df['dest_lon'] = df['destination'].map(lambda x: location_coords.get(x, (np.nan, np.nan))[1])\n",
    "    print(\"coordinate columns have been added.\")\n",
    "else:\n",
    "    # saying that coordinate columns already exist\n",
    "    print(\"coordinate columns already exist.\")\n",
    "\n",
    "# saving the enhanced dataset to a new csv file\n",
    "enhanced_path = \"data/enhanced_gig_data.csv\"\n",
    "df.to_csv(enhanced_path, index=False)\n",
    "print(f\"enhanced dataset has been saved to: {enhanced_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53ae0bbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaned dataset saved successfully.\n"
     ]
    }
   ],
   "source": [
    "# loading the enhanced dataset\n",
    "df = pd.read_csv(\"data/enhanced_gig_data.csv\")\n",
    "\n",
    "# pick only the columns we need\n",
    "columns = [\n",
    "    'datetime', 'hour', 'day', 'month',\n",
    "    'source', 'destination',\n",
    "    'cab_type', 'price', 'distance', 'surge_multiplier',\n",
    "    'gender', 'vehicle_type',\n",
    "    'source_lat', 'source_lon', 'dest_lat', 'dest_lon'\n",
    "]\n",
    "df_clean = df[columns]\n",
    "\n",
    "# drop any rows that have missing values\n",
    "df_clean = df_clean.dropna()\n",
    "\n",
    "# convert datetime column to datetime type\n",
    "df_clean['datetime'] = pd.to_datetime(df_clean['datetime'])\n",
    "\n",
    "# save the cleaned dataset to a new csv file\n",
    "df_clean.to_csv(\"data/cleaned_gig_data.csv\", index=False)\n",
    "\n",
    "print(\"cleaned dataset saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6632cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130ba012",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
